# OoOE

想要获得线性扩展性，最简单的办法是降低每 CPU 的性能，将 CPU 性能降低到与内存速度相同，内存屏障这种现有的同步方法将不再有存在的必要。

the benefit of OoOE processing grows as the instruction pipeline deepens and the speed difference between main memory (or cache memory) and the processor widens. On mordern machines, the processor runs many times faster than the memory, so during the time an in-order processor spends waiting for data to arrive, it could have processed a large number of instructions.

Instruction pipelining is a technique for implementing instruction-level parallelism within a single processor. Pipelining attempts to keep every part of the processor busy with some instruction by dividing incoming instructions into a series of sequential steps (the eponymous "pipeline") performed by different processor unit with different parts of instructions processed in parallel. It allows faster CPU throughput than would otherwise be possible at a given clock rate, but may increase latency due to the added overhead of the pipelining process itself.

最近几年并行计算受到大量关注的主要原因是摩尔定律的终结，摩尔定律带来的单线程程序性能提升也结束了。对硬件设计者来说最严重的一个限制莫过于有限的光速，再一个 1.8GHz 的时钟周期内，光只能在真空中来回传播大约 8cm 远。 在 5GHz 的时钟周期内，这个距离更是降到了 3cm。这个距离对于现代计算机系统的体积来说，还是太小了。更糟糕的是，电子在硅中的运动速度是真空中的 1/30 到 1/3，常见的时钟逻辑结构运行得更慢。比如，内存引用需要将请求发送给系统的其他部分前，等待查找本地缓存操作结束。



第九章

延迟处理技术--RCU





NUMA (Non-uniform memory access)

https://en.wikipedia.org/wiki/Non-uniform_memory_access



Cache coherence

https://en.wikipedia.org/wiki/Cache_coherence



page 314

​	每一个 CPU 执行一个产生内存访问操作的程序。在这个抽象 CPU 中，内存操作顺序是非常随意的，一个 CPU 实际上可以按它喜欢的任意顺序执行内存操作，只要程序因果关系能够得到保持。类似的，编译器也可以按它喜欢的顺序安排指令，只要它不影响程序的可见操作。

Page 315

​	可以期望一个 CPU 有一些最小的保证。

- 在一个给定的 CPU 上，有依赖的内存访问按序运行。
- 在特定 CPU 中，交叉的加载存储操作将在 CPU 内按顺序运行。
- 对单个变量的存储序列将向所有 CPU 呈现出一个单一的序列，虽然这个序列可能不被代码所看见，实际上，在多次运行时，其顺序可能发生变化。
  - 并且有一些事情必须被假设，或者必须不被假设
    - 不能假设：独立的加载和存储操作会按给定顺序运行。
    - 必须假设交叉的内存访问将被合并或者丢弃



Page 397

内存屏障对于高性能和可扩展性来说是必需的坏蛋。这个问题的根源来自于这样的一个事实，CPU 的速度，比 CPU 之间的互联性能及 CPU 试图要访问的内存性能，要快上几个数量级





